{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model Comparison — Interactive\n",
    "\n",
    "Run the conflict experiment with any Groq model directly from this notebook. Models with existing results are skipped automatically.\n",
    "\n",
    "**Prerequisites:** `.env` file with `GROQ_API_KEY` set, and `data/hotpotqa/dev.json` downloaded (run `python test_setup.py` first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "load_dotenv(os.path.join('..', '.env'))\n",
    "\n",
    "from src.data.hotpotqa_loader import HotpotQALoader\n",
    "from src.data.conflict_injector import ConflictInjector\n",
    "from src.inference.groq_client import GroqClient\n",
    "from src.inference.prompt_templates import create_cot_prompt, extract_answer\n",
    "from src.evaluation.metrics import check_answer\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "RESULTS_DIR = '../outputs/results'\n",
    "FIGURES_DIR = '../outputs/figures'\n",
    "N_EXAMPLES = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure Models\n",
    "\n",
    "Edit this list to add/remove models. Any model available on Groq works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    {\"id\": \"llama-3.3-70b-versatile\", \"label\": \"Llama-3.3-70B\"},\n",
    "    {\"id\": \"llama-3.1-8b-instant\",    \"label\": \"Llama-3.1-8B\"},\n",
    "    # Add more models here:\n",
    "    # {\"id\": \"gemma2-9b-it\",           \"label\": \"Gemma2-9B\"},\n",
    "    # {\"id\": \"mixtral-8x7b-32768\",     \"label\": \"Mixtral-8x7B\"},\n",
    "]\n",
    "\n",
    "# Check which models already have results\n",
    "for m in MODELS:\n",
    "    path = os.path.join(RESULTS_DIR, m['id'], 'experiment.json')\n",
    "    exists = os.path.exists(path)\n",
    "    status = 'DONE' if exists else 'PENDING'\n",
    "    print(f\"  [{status}] {m['label']} ({m['id']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = HotpotQALoader()\n",
    "loader.load(path='../data/hotpotqa/dev.json')\n",
    "examples = loader.get_bridge_questions(N_EXAMPLES)\n",
    "print(f'Loaded {len(examples)} bridge questions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Experiments (Only for Missing Models)\n",
    "\n",
    "This cell runs the 3-condition experiment for each model that doesn't have results yet. Models with existing results are skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(results):\n",
    "    metrics = {}\n",
    "    for cond in ['no_conflict', 'conflict_hop1', 'conflict_hop2']:\n",
    "        n = len(results[cond])\n",
    "        if n == 0:\n",
    "            continue\n",
    "        accuracy = sum(r['correct'] for r in results[cond]) / n\n",
    "        if 'conflict' in cond:\n",
    "            cfr = sum(r['followed_context'] for r in results[cond]) / n\n",
    "            por = sum(r['used_parametric'] for r in results[cond]) / n\n",
    "        else:\n",
    "            cfr = por = 0\n",
    "        metrics[cond] = {\n",
    "            'n': n, 'accuracy': accuracy,\n",
    "            'context_following_rate': cfr,\n",
    "            'parametric_override_rate': por\n",
    "        }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def run_single_model(model_id, examples):\n",
    "    \"\"\"Run 3-condition experiment for one model.\"\"\"\n",
    "    result_path = os.path.join(RESULTS_DIR, model_id, 'experiment.json')\n",
    "    if os.path.exists(result_path):\n",
    "        print(f'SKIP {model_id} — results already exist')\n",
    "        return None\n",
    "\n",
    "    print(f'\\nRunning: {model_id}')\n",
    "    injector = ConflictInjector()\n",
    "    client = GroqClient(model=model_id)\n",
    "    results = {'no_conflict': [], 'conflict_hop1': [], 'conflict_hop2': []}\n",
    "\n",
    "    for i, example in enumerate(tqdm(examples, desc=model_id)):\n",
    "        question, doc1, doc2, answer = loader.extract_supporting_facts(example)\n",
    "        if not doc1 or not doc2:\n",
    "            continue\n",
    "\n",
    "        # Condition 1: No Conflict\n",
    "        prompt = create_cot_prompt(question, doc1, doc2)\n",
    "        response = client.generate(prompt)\n",
    "        pred = extract_answer(response)\n",
    "        result = check_answer(pred, answer)\n",
    "        result['condition'] = 'no_conflict'\n",
    "        result['question'] = question\n",
    "        results['no_conflict'].append(result)\n",
    "\n",
    "        # Condition 2: Conflict at Hop 1\n",
    "        mod_doc1, mod_doc2, fake = injector.inject_conflict(question, doc1, doc2, answer, conflict_hop=1)\n",
    "        prompt = create_cot_prompt(question, mod_doc1, mod_doc2)\n",
    "        response = client.generate(prompt)\n",
    "        pred = extract_answer(response)\n",
    "        result = check_answer(pred, answer, fake)\n",
    "        result['condition'] = 'conflict_hop1'\n",
    "        result['question'] = question\n",
    "        results['conflict_hop1'].append(result)\n",
    "\n",
    "        # Condition 3: Conflict at Hop 2\n",
    "        mod_doc1, mod_doc2, fake = injector.inject_conflict(question, doc1, doc2, answer, conflict_hop=2)\n",
    "        prompt = create_cot_prompt(question, mod_doc1, mod_doc2)\n",
    "        response = client.generate(prompt)\n",
    "        pred = extract_answer(response)\n",
    "        result = check_answer(pred, answer, fake)\n",
    "        result['condition'] = 'conflict_hop2'\n",
    "        result['question'] = question\n",
    "        results['conflict_hop2'].append(result)\n",
    "\n",
    "    # Save\n",
    "    metrics = compute_metrics(results)\n",
    "    os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "    with open(result_path, 'w') as f:\n",
    "        json.dump({'metrics': metrics, 'raw_results': results}, f, indent=2)\n",
    "    print(f'Saved: {result_path}')\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Run all pending models\n",
    "for m in MODELS:\n",
    "    run_single_model(m['id'], examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = {}\n",
    "for m in MODELS:\n",
    "    path = os.path.join(RESULTS_DIR, m['id'], 'experiment.json')\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'r') as f:\n",
    "            all_models[m['id']] = {'label': m['label'], 'data': json.load(f)}\n",
    "        print(f\"Loaded: {m['label']}\")\n",
    "    else:\n",
    "        print(f\"Missing: {m['label']}\")\n",
    "\n",
    "conditions = ['no_conflict', 'conflict_hop1', 'conflict_hop2']\n",
    "cond_labels = {'no_conflict': 'No Conflict', 'conflict_hop1': 'Conflict@Hop1', 'conflict_hop2': 'Conflict@Hop2'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for model_id, entry in all_models.items():\n",
    "    metrics = entry['data']['metrics']\n",
    "    for cond in conditions:\n",
    "        if cond not in metrics:\n",
    "            continue\n",
    "        m = metrics[cond]\n",
    "        rows.append({\n",
    "            'Model': entry['label'],\n",
    "            'Condition': cond_labels[cond],\n",
    "            'N': m['n'],\n",
    "            'Accuracy': f\"{m['accuracy']:.1%}\",\n",
    "            'CFR': f\"{m['context_following_rate']:.1%}\" if m['context_following_rate'] > 0 else '-',\n",
    "            'POR': f\"{m['parametric_override_rate']:.1%}\" if m['parametric_override_rate'] > 0 else '-',\n",
    "        })\n",
    "\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Grouped Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "labels = ['No Conflict\\n(Baseline)', 'Conflict at\\nHop 1', 'Conflict at\\nHop 2']\n",
    "x = np.arange(len(labels))\n",
    "n_models = len(all_models)\n",
    "width = 0.7 / n_models\n",
    "colors = ['#3498db', '#e67e22', '#2ecc71', '#9b59b6']\n",
    "\n",
    "for idx, (model_id, entry) in enumerate(all_models.items()):\n",
    "    metrics = entry['data']['metrics']\n",
    "    accs = [metrics[c]['accuracy'] * 100 if c in metrics else 0 for c in conditions]\n",
    "    offset = (idx - (n_models - 1) / 2) * width\n",
    "    bars = ax.bar(x + offset, accs, width, label=entry['label'],\n",
    "                  color=colors[idx % len(colors)], edgecolor='black', linewidth=1)\n",
    "    for bar in bars:\n",
    "        h = bar.get_height()\n",
    "        ax.annotate(f'{h:.1f}%', xy=(bar.get_x() + bar.get_width()/2, h),\n",
    "                    xytext=(0, 3), textcoords='offset points',\n",
    "                    ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Model Comparison: Impact of Knowledge Conflicts', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, fontsize=11)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.legend(fontsize=10)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'model_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved to outputs/figures/model_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistical Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = list(all_models.keys())\n",
    "\n",
    "# Compare all pairs\n",
    "for i in range(len(model_ids)):\n",
    "    for j in range(i + 1, len(model_ids)):\n",
    "        m1, m2 = model_ids[i], model_ids[j]\n",
    "        l1, l2 = all_models[m1]['label'], all_models[m2]['label']\n",
    "        met1, met2 = all_models[m1]['data']['metrics'], all_models[m2]['data']['metrics']\n",
    "\n",
    "        print(f'\\n=== {l1} vs {l2} ===')\n",
    "        stat_rows = []\n",
    "        for cond in conditions:\n",
    "            if cond not in met1 or cond not in met2:\n",
    "                continue\n",
    "            a1, n1 = met1[cond]['accuracy'], met1[cond]['n']\n",
    "            a2, n2 = met2[cond]['accuracy'], met2[cond]['n']\n",
    "            ct = [[int(a1*n1), n1 - int(a1*n1)],\n",
    "                  [int(a2*n2), n2 - int(a2*n2)]]\n",
    "            chi2, p_val, _, _ = stats.chi2_contingency(ct)\n",
    "            stat_rows.append({\n",
    "                'Condition': cond_labels[cond],\n",
    "                l1: f'{a1:.1%}',\n",
    "                l2: f'{a2:.1%}',\n",
    "                'Chi2': f'{chi2:.2f}',\n",
    "                'p-value': f'{p_val:.4f}',\n",
    "                'Significant': 'Yes' if p_val < 0.05 else 'No'\n",
    "            })\n",
    "\n",
    "        display(pd.DataFrame(stat_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Behavior Breakdown (CFR / POR / Other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(all_models), figsize=(6 * len(all_models), 5), sharey=True)\n",
    "if len(all_models) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (model_id, entry) in zip(axes, all_models.items()):\n",
    "    metrics = entry['data']['metrics']\n",
    "    conflict_conds = ['conflict_hop1', 'conflict_hop2']\n",
    "    bar_labels = ['Hop 1', 'Hop 2']\n",
    "\n",
    "    cfr = [metrics[c]['context_following_rate'] * 100 for c in conflict_conds]\n",
    "    por = [metrics[c]['parametric_override_rate'] * 100 for c in conflict_conds]\n",
    "    other = [100 - cfr[i] - por[i] for i in range(len(conflict_conds))]\n",
    "\n",
    "    xp = np.arange(len(bar_labels))\n",
    "    w = 0.5\n",
    "    ax.bar(xp, cfr, w, label='Followed Context (Wrong)', color='#e74c3c')\n",
    "    ax.bar(xp, por, w, bottom=cfr, label='Parametric Override (Correct)', color='#2ecc71')\n",
    "    ax.bar(xp, other, w, bottom=[cfr[i]+por[i] for i in range(len(conflict_conds))],\n",
    "           label='Other/Hallucination', color='#95a5a6')\n",
    "\n",
    "    ax.set_title(entry['label'], fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(xp)\n",
    "    ax.set_xticklabels(bar_labels)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "axes[0].set_ylabel('Percentage (%)')\n",
    "axes[-1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "plt.suptitle('Model Behavior Under Knowledge Conflict', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'behavior_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
